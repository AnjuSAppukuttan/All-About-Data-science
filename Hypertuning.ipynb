{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee4c268",
   "metadata": {},
   "source": [
    "# DL Prediction Enhancing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df7b3d",
   "metadata": {},
   "source": [
    "Try different network architectures like deeper or wider networks, adding dropout or batch normalization for regularization. This can help improve accuracy.\n",
    "\n",
    "Tune hyperparameters like learning rate, batch size, number of epochs etc. through grid search or Bayesian optimization to find the optimal values.\n",
    "\n",
    "Use k-fold cross validation to get a better estimate of model performance and reduce overfitting.\n",
    "\n",
    "Try ensembling models like averaging predictions from multiple models. This can boost performance.\n",
    "\n",
    "Use more advanced optimization algorithms like Adam, RMSprop instead of basic SGD.\n",
    "\n",
    "Add momentum to help accelerate training.\n",
    "\n",
    "Try different activation functions like ReLU, LeakyReLU etc.\n",
    "\n",
    "Use regularization techniques like L1/L2 regularization, dropout to reduce overfitting.\n",
    "\n",
    "Get more data if possible. More data usually results in better deep learning models.\n",
    "\n",
    "Visualize activations and feature maps to better understand what the network is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb7635",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score , classification_report,ConfusionMatrixDisplay,precision_score,recall_score, f1_score,roc_auc_score,roc_curve, balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(3)\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac083ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5797f",
   "metadata": {},
   "source": [
    "# MLP Wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))  \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab04ac",
   "metadata": {},
   "source": [
    "# MLP Tune Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search \n",
    "batch_size = [16, 32, 64]\n",
    "learning_rate = [0.01, 0.001, 0.0001]\n",
    "param_grid = dict(batch_size=batch_size, learning_rate=learning_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Bayesian optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def fitness(learning_rate, epochs):\n",
    "    model.compile(learning_rate=learning_rate, epochs=epochs) \n",
    "    history = model.fit(X_train, y_train)\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "bayes_opt = BayesianOptimization(fitness, {'learning_rate': (0.001, 0.1),  \n",
    "                                           'epochs': (5, 30)})\n",
    "bayes_opt.maximize()\n",
    "best_params = bayes_opt.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175a2c8",
   "metadata": {},
   "source": [
    "# Using Score function for Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "x =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense = Dense(num_classes, activation='sigmoid')        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = Flatten()(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        accuracy = np.mean(np.equal(np.round(y_pred), y))\n",
    "        return accuracy\n",
    "    \n",
    "model = MyModel(num_classes=2)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b384ad",
   "metadata": {},
   "source": [
    "# Using Gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad679b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Load the credit card dataset\n",
    "data = pd.read_csv('Diabetes_dataset.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Diabetes', axis=1)\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the generator and discriminator networks\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Generator network\n",
    "generator_input = Input(shape=(X_train.shape[1],))\n",
    "generator_hidden = Dense(128, activation='relu')(generator_input)\n",
    "generator_hidden = Dense(64, activation='relu')(generator_hidden)\n",
    "generator_output = Dense(X_train.shape[1], activation='tanh')(generator_hidden)\n",
    "\n",
    "generator = Model(inputs=generator_input, outputs=generator_output)\n",
    "\n",
    "# Discriminator network\n",
    "discriminator_input = Input(shape=(X_train.shape[1],))\n",
    "discriminator_hidden = Dense(128, activation='relu')(discriminator_input)\n",
    "discriminator_hidden = Dense(64, activation='relu')(discriminator_hidden)\n",
    "discriminator_output = Dense(1, activation='sigmoid')(discriminator_hidden)\n",
    "\n",
    "discriminator = Model(inputs=discriminator_input, outputs=discriminator_output)\n",
    "\n",
    "# Compile the generator and discriminator\n",
    "generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the generator and discriminator\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Generate synthetic data\n",
    "    synthetic_data = generator.predict(np.random.normal(size=(batch_size, X_train.shape[1])))\n",
    "\n",
    "    # Combine synthetic and real data\n",
    "    X_combined = np.concatenate([X_train[:batch_size], synthetic_data], axis=0)\n",
    "    y_combined = np.concatenate([np.ones(batch_size), np.zeros(batch_size)], axis=0)\n",
    "\n",
    "    # Train the discriminator\n",
    "    discriminator.train_on_batch(X_combined, y_combined)\n",
    "\n",
    "    # Train the generator\n",
    "    noise = np.random.normal(size=(batch_size, X_train.shape[1]))\n",
    "    generator_labels = np.ones(batch_size)\n",
    "    discriminator.trainable = False\n",
    "    generator.train_on_batch(noise, generator_labels)\n",
    "    discriminator.trainable = True\n",
    "\n",
    "# Augment the training data with synthetic data\n",
    "X_augmented = np.concatenate([X_train, synthetic_data], axis=0)\n",
    "y_augmented = np.concatenate([y_train, np.ones(batch_size)], axis=0)\n",
    "\n",
    "# Train the model with augmented data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_augmented, y_augmented)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1 score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9f571",
   "metadata": {},
   "source": [
    "# K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "histories = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index] \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "    #history = model.fit(X_train, y_train) \n",
    "    histories.append(history)\n",
    "\n",
    "performance = [history.history['val_accuracy'][-1] for history in histories]\n",
    "print(\"Average 5-Fold Validation Accuracy: %.2f\" % (sum(performance)/len(performance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a1879",
   "metadata": {},
   "source": [
    "# Two model Combine Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16deceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two different models\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=17, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2 = Sequential() \n",
    "model2.add(Dense(32, input_dim=17, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Make predictions and average\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "\n",
    "y_pred = (pred1 + pred2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cce9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model1.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e0b59",
   "metadata": {},
   "source": [
    "# Learning rate adding to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b56497",
   "metadata": {},
   "source": [
    "# Adding momentum and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a737a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f9ac2",
   "metadata": {},
   "source": [
    "# LeakyReLU Activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "model = Sequential() \n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "model.add(Dense(64, activation=LeakyReLU(alpha=0.05)))\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4a77e",
   "metadata": {},
   "source": [
    "# Adding Regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the MLP model\n",
    "\n",
    "model = Sequential() \n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.01)))  \n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation=LeakyReLU(alpha=0.05)))\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
